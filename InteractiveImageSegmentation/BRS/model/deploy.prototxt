name: "BRS_DENSENET_121"
force_backward: true

layer { 
  name: 'data' 
  type: 'Input' 
  top: 'data' 
  input_param { 
    shape: { dim: 1 dim: 3 dim: 480 dim: 480 } 
  } 
}

layer { 
  name: 'iact' 
  type: 'Input' 
  top: 'iact' 
  input_param { 
    shape: { dim: 1 dim: 2 dim: 480 dim: 480 } 
  } 
}
# ========================================================
# Concatenation of input data
# ========================================================

layer {
  name: "concat_input"
  bottom: "data"
  bottom: "iact"
  top: "concat_input"
  type: "Concat"
  concat_param {
    concat_dim: 1
  }
}


# =======================================================
# DenseNet Encoder
# =======================================================
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "concat_input"
  top: "conv1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 3
    kernel_size: 7
    stride: 2
    weight_filler { type: "gaussian" std: 0.1 } 
  }
}
layer {
  name: "conv1/bn"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv1/scale"
  type: "Scale"
  bottom: "conv1/bn"
  top: "conv1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1/bn"
  top: "conv1/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    pad: 1
    ceil_mode: false
  }
}
layer {
  name: "conv2_1/x1/bn"
  type: "BatchNorm"
  bottom: "pool1"
  top: "conv2_1/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv2_1/x1/scale"
  type: "Scale"
  bottom: "conv2_1/x1/bn"
  top: "conv2_1/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_1/x1"
  type: "ReLU"
  bottom: "conv2_1/x1/bn"
  top: "conv2_1/x1/bn"
}
layer {
  name: "conv2_1/x1"
  type: "Convolution"
  bottom: "conv2_1/x1/bn"
  top: "conv2_1/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv2_1/x2/bn"
  type: "BatchNorm"
  bottom: "conv2_1/x1"
  top: "conv2_1/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv2_1/x2/scale"
  type: "Scale"
  bottom: "conv2_1/x2/bn"
  top: "conv2_1/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_1/x2"
  type: "ReLU"
  bottom: "conv2_1/x2/bn"
  top: "conv2_1/x2/bn"
}
layer {
  name: "conv2_1/x2"
  type: "Convolution"
  bottom: "conv2_1/x2/bn"
  top: "conv2_1/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_2_1"
  type: "Concat"
  bottom: "pool1"
  bottom: "conv2_1/x2"
  top: "concat_2_1"
}
layer {
  name: "conv2_2/x1/bn"
  type: "BatchNorm"
  bottom: "concat_2_1"
  top: "conv2_2/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv2_2/x1/scale"
  type: "Scale"
  bottom: "conv2_2/x1/bn"
  top: "conv2_2/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2/x1"
  type: "ReLU"
  bottom: "conv2_2/x1/bn"
  top: "conv2_2/x1/bn"
}
layer {
  name: "conv2_2/x1"
  type: "Convolution"
  bottom: "conv2_2/x1/bn"
  top: "conv2_2/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv2_2/x2/bn"
  type: "BatchNorm"
  bottom: "conv2_2/x1"
  top: "conv2_2/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv2_2/x2/scale"
  type: "Scale"
  bottom: "conv2_2/x2/bn"
  top: "conv2_2/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2/x2"
  type: "ReLU"
  bottom: "conv2_2/x2/bn"
  top: "conv2_2/x2/bn"
}
layer {
  name: "conv2_2/x2"
  type: "Convolution"
  bottom: "conv2_2/x2/bn"
  top: "conv2_2/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_2_2"
  type: "Concat"
  bottom: "concat_2_1"
  bottom: "conv2_2/x2"
  top: "concat_2_2"
}
layer {
  name: "conv2_3/x1/bn"
  type: "BatchNorm"
  bottom: "concat_2_2"
  top: "conv2_3/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv2_3/x1/scale"
  type: "Scale"
  bottom: "conv2_3/x1/bn"
  top: "conv2_3/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_3/x1"
  type: "ReLU"
  bottom: "conv2_3/x1/bn"
  top: "conv2_3/x1/bn"
}
layer {
  name: "conv2_3/x1"
  type: "Convolution"
  bottom: "conv2_3/x1/bn"
  top: "conv2_3/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv2_3/x2/bn"
  type: "BatchNorm"
  bottom: "conv2_3/x1"
  top: "conv2_3/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv2_3/x2/scale"
  type: "Scale"
  bottom: "conv2_3/x2/bn"
  top: "conv2_3/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_3/x2"
  type: "ReLU"
  bottom: "conv2_3/x2/bn"
  top: "conv2_3/x2/bn"
}
layer {
  name: "conv2_3/x2"
  type: "Convolution"
  bottom: "conv2_3/x2/bn"
  top: "conv2_3/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_2_3"
  type: "Concat"
  bottom: "concat_2_2"
  bottom: "conv2_3/x2"
  top: "concat_2_3"
}
layer {
  name: "conv2_4/x1/bn"
  type: "BatchNorm"
  bottom: "concat_2_3"
  top: "conv2_4/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv2_4/x1/scale"
  type: "Scale"
  bottom: "conv2_4/x1/bn"
  top: "conv2_4/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_4/x1"
  type: "ReLU"
  bottom: "conv2_4/x1/bn"
  top: "conv2_4/x1/bn"
}
layer {
  name: "conv2_4/x1"
  type: "Convolution"
  bottom: "conv2_4/x1/bn"
  top: "conv2_4/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv2_4/x2/bn"
  type: "BatchNorm"
  bottom: "conv2_4/x1"
  top: "conv2_4/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv2_4/x2/scale"
  type: "Scale"
  bottom: "conv2_4/x2/bn"
  top: "conv2_4/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_4/x2"
  type: "ReLU"
  bottom: "conv2_4/x2/bn"
  top: "conv2_4/x2/bn"
}
layer {
  name: "conv2_4/x2"
  type: "Convolution"
  bottom: "conv2_4/x2/bn"
  top: "conv2_4/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_2_4"
  type: "Concat"
  bottom: "concat_2_3"
  bottom: "conv2_4/x2"
  top: "concat_2_4"
}
layer {
  name: "conv2_5/x1/bn"
  type: "BatchNorm"
  bottom: "concat_2_4"
  top: "conv2_5/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv2_5/x1/scale"
  type: "Scale"
  bottom: "conv2_5/x1/bn"
  top: "conv2_5/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_5/x1"
  type: "ReLU"
  bottom: "conv2_5/x1/bn"
  top: "conv2_5/x1/bn"
}
layer {
  name: "conv2_5/x1"
  type: "Convolution"
  bottom: "conv2_5/x1/bn"
  top: "conv2_5/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv2_5/x2/bn"
  type: "BatchNorm"
  bottom: "conv2_5/x1"
  top: "conv2_5/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv2_5/x2/scale"
  type: "Scale"
  bottom: "conv2_5/x2/bn"
  top: "conv2_5/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_5/x2"
  type: "ReLU"
  bottom: "conv2_5/x2/bn"
  top: "conv2_5/x2/bn"
}
layer {
  name: "conv2_5/x2"
  type: "Convolution"
  bottom: "conv2_5/x2/bn"
  top: "conv2_5/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_2_5"
  type: "Concat"
  bottom: "concat_2_4"
  bottom: "conv2_5/x2"
  top: "concat_2_5"
}
layer {
  name: "conv2_6/x1/bn"
  type: "BatchNorm"
  bottom: "concat_2_5"
  top: "conv2_6/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv2_6/x1/scale"
  type: "Scale"
  bottom: "conv2_6/x1/bn"
  top: "conv2_6/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_6/x1"
  type: "ReLU"
  bottom: "conv2_6/x1/bn"
  top: "conv2_6/x1/bn"
}
layer {
  name: "conv2_6/x1"
  type: "Convolution"
  bottom: "conv2_6/x1/bn"
  top: "conv2_6/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv2_6/x2/bn"
  type: "BatchNorm"
  bottom: "conv2_6/x1"
  top: "conv2_6/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv2_6/x2/scale"
  type: "Scale"
  bottom: "conv2_6/x2/bn"
  top: "conv2_6/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_6/x2"
  type: "ReLU"
  bottom: "conv2_6/x2/bn"
  top: "conv2_6/x2/bn"
}
layer {
  name: "conv2_6/x2"
  type: "Convolution"
  bottom: "conv2_6/x2/bn"
  top: "conv2_6/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_2_6"
  type: "Concat"
  bottom: "concat_2_5"
  bottom: "conv2_6/x2"
  top: "concat_2_6"
}
layer {
  name: "conv2_blk/bn"
  type: "BatchNorm"
  bottom: "concat_2_6"
  top: "conv2_blk/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv2_blk/scale"
  type: "Scale"
  bottom: "conv2_blk/bn"
  top: "conv2_blk/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_blk"
  type: "ReLU"
  bottom: "conv2_blk/bn"
  top: "conv2_blk/bn"
}
layer {
  name: "conv2_blk"
  type: "Convolution"
  bottom: "conv2_blk/bn"
  top: "conv2_blk"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2_blk"
  top: "pool2"
  pooling_param {
    pool: AVE
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3_1/x1/bn"
  type: "BatchNorm"
  bottom: "pool2"
  top: "conv3_1/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv3_1/x1/scale"
  type: "Scale"
  bottom: "conv3_1/x1/bn"
  top: "conv3_1/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_1/x1"
  type: "ReLU"
  bottom: "conv3_1/x1/bn"
  top: "conv3_1/x1/bn"
}
layer {
  name: "conv3_1/x1"
  type: "Convolution"
  bottom: "conv3_1/x1/bn"
  top: "conv3_1/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv3_1/x2/bn"
  type: "BatchNorm"
  bottom: "conv3_1/x1"
  top: "conv3_1/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv3_1/x2/scale"
  type: "Scale"
  bottom: "conv3_1/x2/bn"
  top: "conv3_1/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_1/x2"
  type: "ReLU"
  bottom: "conv3_1/x2/bn"
  top: "conv3_1/x2/bn"
}
layer {
  name: "conv3_1/x2"
  type: "Convolution"
  bottom: "conv3_1/x2/bn"
  top: "conv3_1/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_3_1"
  type: "Concat"
  bottom: "pool2"
  bottom: "conv3_1/x2"
  top: "concat_3_1"
}
layer {
  name: "conv3_2/x1/bn"
  type: "BatchNorm"
  bottom: "concat_3_1"
  top: "conv3_2/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv3_2/x1/scale"
  type: "Scale"
  bottom: "conv3_2/x1/bn"
  top: "conv3_2/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_2/x1"
  type: "ReLU"
  bottom: "conv3_2/x1/bn"
  top: "conv3_2/x1/bn"
}
layer {
  name: "conv3_2/x1"
  type: "Convolution"
  bottom: "conv3_2/x1/bn"
  top: "conv3_2/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv3_2/x2/bn"
  type: "BatchNorm"
  bottom: "conv3_2/x1"
  top: "conv3_2/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv3_2/x2/scale"
  type: "Scale"
  bottom: "conv3_2/x2/bn"
  top: "conv3_2/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_2/x2"
  type: "ReLU"
  bottom: "conv3_2/x2/bn"
  top: "conv3_2/x2/bn"
}
layer {
  name: "conv3_2/x2"
  type: "Convolution"
  bottom: "conv3_2/x2/bn"
  top: "conv3_2/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_3_2"
  type: "Concat"
  bottom: "concat_3_1"
  bottom: "conv3_2/x2"
  top: "concat_3_2"
}
layer {
  name: "conv3_3/x1/bn"
  type: "BatchNorm"
  bottom: "concat_3_2"
  top: "conv3_3/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv3_3/x1/scale"
  type: "Scale"
  bottom: "conv3_3/x1/bn"
  top: "conv3_3/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_3/x1"
  type: "ReLU"
  bottom: "conv3_3/x1/bn"
  top: "conv3_3/x1/bn"
}
layer {
  name: "conv3_3/x1"
  type: "Convolution"
  bottom: "conv3_3/x1/bn"
  top: "conv3_3/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv3_3/x2/bn"
  type: "BatchNorm"
  bottom: "conv3_3/x1"
  top: "conv3_3/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv3_3/x2/scale"
  type: "Scale"
  bottom: "conv3_3/x2/bn"
  top: "conv3_3/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_3/x2"
  type: "ReLU"
  bottom: "conv3_3/x2/bn"
  top: "conv3_3/x2/bn"
}
layer {
  name: "conv3_3/x2"
  type: "Convolution"
  bottom: "conv3_3/x2/bn"
  top: "conv3_3/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_3_3"
  type: "Concat"
  bottom: "concat_3_2"
  bottom: "conv3_3/x2"
  top: "concat_3_3"
}
layer {
  name: "conv3_4/x1/bn"
  type: "BatchNorm"
  bottom: "concat_3_3"
  top: "conv3_4/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv3_4/x1/scale"
  type: "Scale"
  bottom: "conv3_4/x1/bn"
  top: "conv3_4/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_4/x1"
  type: "ReLU"
  bottom: "conv3_4/x1/bn"
  top: "conv3_4/x1/bn"
}
layer {
  name: "conv3_4/x1"
  type: "Convolution"
  bottom: "conv3_4/x1/bn"
  top: "conv3_4/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv3_4/x2/bn"
  type: "BatchNorm"
  bottom: "conv3_4/x1"
  top: "conv3_4/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv3_4/x2/scale"
  type: "Scale"
  bottom: "conv3_4/x2/bn"
  top: "conv3_4/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_4/x2"
  type: "ReLU"
  bottom: "conv3_4/x2/bn"
  top: "conv3_4/x2/bn"
}
layer {
  name: "conv3_4/x2"
  type: "Convolution"
  bottom: "conv3_4/x2/bn"
  top: "conv3_4/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_3_4"
  type: "Concat"
  bottom: "concat_3_3"
  bottom: "conv3_4/x2"
  top: "concat_3_4"
}
layer {
  name: "conv3_5/x1/bn"
  type: "BatchNorm"
  bottom: "concat_3_4"
  top: "conv3_5/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv3_5/x1/scale"
  type: "Scale"
  bottom: "conv3_5/x1/bn"
  top: "conv3_5/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_5/x1"
  type: "ReLU"
  bottom: "conv3_5/x1/bn"
  top: "conv3_5/x1/bn"
}
layer {
  name: "conv3_5/x1"
  type: "Convolution"
  bottom: "conv3_5/x1/bn"
  top: "conv3_5/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv3_5/x2/bn"
  type: "BatchNorm"
  bottom: "conv3_5/x1"
  top: "conv3_5/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv3_5/x2/scale"
  type: "Scale"
  bottom: "conv3_5/x2/bn"
  top: "conv3_5/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_5/x2"
  type: "ReLU"
  bottom: "conv3_5/x2/bn"
  top: "conv3_5/x2/bn"
}
layer {
  name: "conv3_5/x2"
  type: "Convolution"
  bottom: "conv3_5/x2/bn"
  top: "conv3_5/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_3_5"
  type: "Concat"
  bottom: "concat_3_4"
  bottom: "conv3_5/x2"
  top: "concat_3_5"
}
layer {
  name: "conv3_6/x1/bn"
  type: "BatchNorm"
  bottom: "concat_3_5"
  top: "conv3_6/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv3_6/x1/scale"
  type: "Scale"
  bottom: "conv3_6/x1/bn"
  top: "conv3_6/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_6/x1"
  type: "ReLU"
  bottom: "conv3_6/x1/bn"
  top: "conv3_6/x1/bn"
}
layer {
  name: "conv3_6/x1"
  type: "Convolution"
  bottom: "conv3_6/x1/bn"
  top: "conv3_6/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv3_6/x2/bn"
  type: "BatchNorm"
  bottom: "conv3_6/x1"
  top: "conv3_6/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv3_6/x2/scale"
  type: "Scale"
  bottom: "conv3_6/x2/bn"
  top: "conv3_6/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_6/x2"
  type: "ReLU"
  bottom: "conv3_6/x2/bn"
  top: "conv3_6/x2/bn"
}
layer {
  name: "conv3_6/x2"
  type: "Convolution"
  bottom: "conv3_6/x2/bn"
  top: "conv3_6/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_3_6"
  type: "Concat"
  bottom: "concat_3_5"
  bottom: "conv3_6/x2"
  top: "concat_3_6"
}
layer {
  name: "conv3_7/x1/bn"
  type: "BatchNorm"
  bottom: "concat_3_6"
  top: "conv3_7/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv3_7/x1/scale"
  type: "Scale"
  bottom: "conv3_7/x1/bn"
  top: "conv3_7/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_7/x1"
  type: "ReLU"
  bottom: "conv3_7/x1/bn"
  top: "conv3_7/x1/bn"
}
layer {
  name: "conv3_7/x1"
  type: "Convolution"
  bottom: "conv3_7/x1/bn"
  top: "conv3_7/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv3_7/x2/bn"
  type: "BatchNorm"
  bottom: "conv3_7/x1"
  top: "conv3_7/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv3_7/x2/scale"
  type: "Scale"
  bottom: "conv3_7/x2/bn"
  top: "conv3_7/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_7/x2"
  type: "ReLU"
  bottom: "conv3_7/x2/bn"
  top: "conv3_7/x2/bn"
}
layer {
  name: "conv3_7/x2"
  type: "Convolution"
  bottom: "conv3_7/x2/bn"
  top: "conv3_7/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_3_7"
  type: "Concat"
  bottom: "concat_3_6"
  bottom: "conv3_7/x2"
  top: "concat_3_7"
}
layer {
  name: "conv3_8/x1/bn"
  type: "BatchNorm"
  bottom: "concat_3_7"
  top: "conv3_8/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv3_8/x1/scale"
  type: "Scale"
  bottom: "conv3_8/x1/bn"
  top: "conv3_8/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_8/x1"
  type: "ReLU"
  bottom: "conv3_8/x1/bn"
  top: "conv3_8/x1/bn"
}
layer {
  name: "conv3_8/x1"
  type: "Convolution"
  bottom: "conv3_8/x1/bn"
  top: "conv3_8/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv3_8/x2/bn"
  type: "BatchNorm"
  bottom: "conv3_8/x1"
  top: "conv3_8/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv3_8/x2/scale"
  type: "Scale"
  bottom: "conv3_8/x2/bn"
  top: "conv3_8/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_8/x2"
  type: "ReLU"
  bottom: "conv3_8/x2/bn"
  top: "conv3_8/x2/bn"
}
layer {
  name: "conv3_8/x2"
  type: "Convolution"
  bottom: "conv3_8/x2/bn"
  top: "conv3_8/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_3_8"
  type: "Concat"
  bottom: "concat_3_7"
  bottom: "conv3_8/x2"
  top: "concat_3_8"
}
layer {
  name: "conv3_9/x1/bn"
  type: "BatchNorm"
  bottom: "concat_3_8"
  top: "conv3_9/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv3_9/x1/scale"
  type: "Scale"
  bottom: "conv3_9/x1/bn"
  top: "conv3_9/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_9/x1"
  type: "ReLU"
  bottom: "conv3_9/x1/bn"
  top: "conv3_9/x1/bn"
}
layer {
  name: "conv3_9/x1"
  type: "Convolution"
  bottom: "conv3_9/x1/bn"
  top: "conv3_9/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv3_9/x2/bn"
  type: "BatchNorm"
  bottom: "conv3_9/x1"
  top: "conv3_9/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv3_9/x2/scale"
  type: "Scale"
  bottom: "conv3_9/x2/bn"
  top: "conv3_9/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_9/x2"
  type: "ReLU"
  bottom: "conv3_9/x2/bn"
  top: "conv3_9/x2/bn"
}
layer {
  name: "conv3_9/x2"
  type: "Convolution"
  bottom: "conv3_9/x2/bn"
  top: "conv3_9/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_3_9"
  type: "Concat"
  bottom: "concat_3_8"
  bottom: "conv3_9/x2"
  top: "concat_3_9"
}
layer {
  name: "conv3_10/x1/bn"
  type: "BatchNorm"
  bottom: "concat_3_9"
  top: "conv3_10/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv3_10/x1/scale"
  type: "Scale"
  bottom: "conv3_10/x1/bn"
  top: "conv3_10/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_10/x1"
  type: "ReLU"
  bottom: "conv3_10/x1/bn"
  top: "conv3_10/x1/bn"
}
layer {
  name: "conv3_10/x1"
  type: "Convolution"
  bottom: "conv3_10/x1/bn"
  top: "conv3_10/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv3_10/x2/bn"
  type: "BatchNorm"
  bottom: "conv3_10/x1"
  top: "conv3_10/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv3_10/x2/scale"
  type: "Scale"
  bottom: "conv3_10/x2/bn"
  top: "conv3_10/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_10/x2"
  type: "ReLU"
  bottom: "conv3_10/x2/bn"
  top: "conv3_10/x2/bn"
}
layer {
  name: "conv3_10/x2"
  type: "Convolution"
  bottom: "conv3_10/x2/bn"
  top: "conv3_10/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_3_10"
  type: "Concat"
  bottom: "concat_3_9"
  bottom: "conv3_10/x2"
  top: "concat_3_10"
}
layer {
  name: "conv3_11/x1/bn"
  type: "BatchNorm"
  bottom: "concat_3_10"
  top: "conv3_11/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv3_11/x1/scale"
  type: "Scale"
  bottom: "conv3_11/x1/bn"
  top: "conv3_11/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_11/x1"
  type: "ReLU"
  bottom: "conv3_11/x1/bn"
  top: "conv3_11/x1/bn"
}
layer {
  name: "conv3_11/x1"
  type: "Convolution"
  bottom: "conv3_11/x1/bn"
  top: "conv3_11/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv3_11/x2/bn"
  type: "BatchNorm"
  bottom: "conv3_11/x1"
  top: "conv3_11/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv3_11/x2/scale"
  type: "Scale"
  bottom: "conv3_11/x2/bn"
  top: "conv3_11/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_11/x2"
  type: "ReLU"
  bottom: "conv3_11/x2/bn"
  top: "conv3_11/x2/bn"
}
layer {
  name: "conv3_11/x2"
  type: "Convolution"
  bottom: "conv3_11/x2/bn"
  top: "conv3_11/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_3_11"
  type: "Concat"
  bottom: "concat_3_10"
  bottom: "conv3_11/x2"
  top: "concat_3_11"
}
layer {
  name: "conv3_12/x1/bn"
  type: "BatchNorm"
  bottom: "concat_3_11"
  top: "conv3_12/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv3_12/x1/scale"
  type: "Scale"
  bottom: "conv3_12/x1/bn"
  top: "conv3_12/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_12/x1"
  type: "ReLU"
  bottom: "conv3_12/x1/bn"
  top: "conv3_12/x1/bn"
}
layer {
  name: "conv3_12/x1"
  type: "Convolution"
  bottom: "conv3_12/x1/bn"
  top: "conv3_12/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv3_12/x2/bn"
  type: "BatchNorm"
  bottom: "conv3_12/x1"
  top: "conv3_12/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv3_12/x2/scale"
  type: "Scale"
  bottom: "conv3_12/x2/bn"
  top: "conv3_12/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_12/x2"
  type: "ReLU"
  bottom: "conv3_12/x2/bn"
  top: "conv3_12/x2/bn"
}
layer {
  name: "conv3_12/x2"
  type: "Convolution"
  bottom: "conv3_12/x2/bn"
  top: "conv3_12/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_3_12"
  type: "Concat"
  bottom: "concat_3_11"
  bottom: "conv3_12/x2"
  top: "concat_3_12"
}
layer {
  name: "conv3_blk/bn"
  type: "BatchNorm"
  bottom: "concat_3_12"
  top: "conv3_blk/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv3_blk/scale"
  type: "Scale"
  bottom: "conv3_blk/bn"
  top: "conv3_blk/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_blk"
  type: "ReLU"
  bottom: "conv3_blk/bn"
  top: "conv3_blk/bn"
}
layer {
  name: "conv3_blk"
  type: "Convolution"
  bottom: "conv3_blk/bn"
  top: "conv3_blk"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 256
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3_blk"
  top: "pool3"
  pooling_param {
    pool: AVE
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4_1/x1/bn"
  type: "BatchNorm"
  bottom: "pool3"
  top: "conv4_1/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv4_1/x1/scale"
  type: "Scale"
  bottom: "conv4_1/x1/bn"
  top: "conv4_1/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1/x1"
  type: "ReLU"
  bottom: "conv4_1/x1/bn"
  top: "conv4_1/x1/bn"
}
layer {
  name: "conv4_1/x1"
  type: "Convolution"
  bottom: "conv4_1/x1/bn"
  top: "conv4_1/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_1/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_1/x1"
  top: "conv4_1/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv4_1/x2/scale"
  type: "Scale"
  bottom: "conv4_1/x2/bn"
  top: "conv4_1/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1/x2"
  type: "ReLU"
  bottom: "conv4_1/x2/bn"
  top: "conv4_1/x2/bn"
}
layer {
  name: "conv4_1/x2"
  type: "Convolution"
  bottom: "conv4_1/x2/bn"
  top: "conv4_1/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_1"
  type: "Concat"
  bottom: "pool3"
  bottom: "conv4_1/x2"
  top: "concat_4_1"
}
layer {
  name: "conv4_2/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_1"
  top: "conv4_2/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv4_2/x1/scale"
  type: "Scale"
  bottom: "conv4_2/x1/bn"
  top: "conv4_2/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2/x1"
  type: "ReLU"
  bottom: "conv4_2/x1/bn"
  top: "conv4_2/x1/bn"
}
layer {
  name: "conv4_2/x1"
  type: "Convolution"
  bottom: "conv4_2/x1/bn"
  top: "conv4_2/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_2/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_2/x1"
  top: "conv4_2/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv4_2/x2/scale"
  type: "Scale"
  bottom: "conv4_2/x2/bn"
  top: "conv4_2/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2/x2"
  type: "ReLU"
  bottom: "conv4_2/x2/bn"
  top: "conv4_2/x2/bn"
}
layer {
  name: "conv4_2/x2"
  type: "Convolution"
  bottom: "conv4_2/x2/bn"
  top: "conv4_2/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_2"
  type: "Concat"
  bottom: "concat_4_1"
  bottom: "conv4_2/x2"
  top: "concat_4_2"
}
layer {
  name: "conv4_3/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_2"
  top: "conv4_3/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv4_3/x1/scale"
  type: "Scale"
  bottom: "conv4_3/x1/bn"
  top: "conv4_3/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_3/x1"
  type: "ReLU"
  bottom: "conv4_3/x1/bn"
  top: "conv4_3/x1/bn"
}
layer {
  name: "conv4_3/x1"
  type: "Convolution"
  bottom: "conv4_3/x1/bn"
  top: "conv4_3/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_3/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_3/x1"
  top: "conv4_3/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv4_3/x2/scale"
  type: "Scale"
  bottom: "conv4_3/x2/bn"
  top: "conv4_3/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_3/x2"
  type: "ReLU"
  bottom: "conv4_3/x2/bn"
  top: "conv4_3/x2/bn"
}
layer {
  name: "conv4_3/x2"
  type: "Convolution"
  bottom: "conv4_3/x2/bn"
  top: "conv4_3/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_3"
  type: "Concat"
  bottom: "concat_4_2"
  bottom: "conv4_3/x2"
  top: "concat_4_3"
}
layer {
  name: "conv4_4/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_3"
  top: "conv4_4/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv4_4/x1/scale"
  type: "Scale"
  bottom: "conv4_4/x1/bn"
  top: "conv4_4/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_4/x1"
  type: "ReLU"
  bottom: "conv4_4/x1/bn"
  top: "conv4_4/x1/bn"
}
layer {
  name: "conv4_4/x1"
  type: "Convolution"
  bottom: "conv4_4/x1/bn"
  top: "conv4_4/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_4/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_4/x1"
  top: "conv4_4/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv4_4/x2/scale"
  type: "Scale"
  bottom: "conv4_4/x2/bn"
  top: "conv4_4/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_4/x2"
  type: "ReLU"
  bottom: "conv4_4/x2/bn"
  top: "conv4_4/x2/bn"
}
layer {
  name: "conv4_4/x2"
  type: "Convolution"
  bottom: "conv4_4/x2/bn"
  top: "conv4_4/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_4"
  type: "Concat"
  bottom: "concat_4_3"
  bottom: "conv4_4/x2"
  top: "concat_4_4"
}
layer {
  name: "conv4_5/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_4"
  top: "conv4_5/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv4_5/x1/scale"
  type: "Scale"
  bottom: "conv4_5/x1/bn"
  top: "conv4_5/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_5/x1"
  type: "ReLU"
  bottom: "conv4_5/x1/bn"
  top: "conv4_5/x1/bn"
}
layer {
  name: "conv4_5/x1"
  type: "Convolution"
  bottom: "conv4_5/x1/bn"
  top: "conv4_5/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_5/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_5/x1"
  top: "conv4_5/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv4_5/x2/scale"
  type: "Scale"
  bottom: "conv4_5/x2/bn"
  top: "conv4_5/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_5/x2"
  type: "ReLU"
  bottom: "conv4_5/x2/bn"
  top: "conv4_5/x2/bn"
}
layer {
  name: "conv4_5/x2"
  type: "Convolution"
  bottom: "conv4_5/x2/bn"
  top: "conv4_5/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_5"
  type: "Concat"
  bottom: "concat_4_4"
  bottom: "conv4_5/x2"
  top: "concat_4_5"
}
layer {
  name: "conv4_6/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_5"
  top: "conv4_6/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv4_6/x1/scale"
  type: "Scale"
  bottom: "conv4_6/x1/bn"
  top: "conv4_6/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_6/x1"
  type: "ReLU"
  bottom: "conv4_6/x1/bn"
  top: "conv4_6/x1/bn"
}
layer {
  name: "conv4_6/x1"
  type: "Convolution"
  bottom: "conv4_6/x1/bn"
  top: "conv4_6/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_6/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_6/x1"
  top: "conv4_6/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv4_6/x2/scale"
  type: "Scale"
  bottom: "conv4_6/x2/bn"
  top: "conv4_6/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_6/x2"
  type: "ReLU"
  bottom: "conv4_6/x2/bn"
  top: "conv4_6/x2/bn"
}
layer {
  name: "conv4_6/x2"
  type: "Convolution"
  bottom: "conv4_6/x2/bn"
  top: "conv4_6/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_6"
  type: "Concat"
  bottom: "concat_4_5"
  bottom: "conv4_6/x2"
  top: "concat_4_6"
}
layer {
  name: "conv4_7/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_6"
  top: "conv4_7/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv4_7/x1/scale"
  type: "Scale"
  bottom: "conv4_7/x1/bn"
  top: "conv4_7/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_7/x1"
  type: "ReLU"
  bottom: "conv4_7/x1/bn"
  top: "conv4_7/x1/bn"
}
layer {
  name: "conv4_7/x1"
  type: "Convolution"
  bottom: "conv4_7/x1/bn"
  top: "conv4_7/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_7/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_7/x1"
  top: "conv4_7/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv4_7/x2/scale"
  type: "Scale"
  bottom: "conv4_7/x2/bn"
  top: "conv4_7/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_7/x2"
  type: "ReLU"
  bottom: "conv4_7/x2/bn"
  top: "conv4_7/x2/bn"
}
layer {
  name: "conv4_7/x2"
  type: "Convolution"
  bottom: "conv4_7/x2/bn"
  top: "conv4_7/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_7"
  type: "Concat"
  bottom: "concat_4_6"
  bottom: "conv4_7/x2"
  top: "concat_4_7"
}
layer {
  name: "conv4_8/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_7"
  top: "conv4_8/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv4_8/x1/scale"
  type: "Scale"
  bottom: "conv4_8/x1/bn"
  top: "conv4_8/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_8/x1"
  type: "ReLU"
  bottom: "conv4_8/x1/bn"
  top: "conv4_8/x1/bn"
}
layer {
  name: "conv4_8/x1"
  type: "Convolution"
  bottom: "conv4_8/x1/bn"
  top: "conv4_8/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_8/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_8/x1"
  top: "conv4_8/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv4_8/x2/scale"
  type: "Scale"
  bottom: "conv4_8/x2/bn"
  top: "conv4_8/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_8/x2"
  type: "ReLU"
  bottom: "conv4_8/x2/bn"
  top: "conv4_8/x2/bn"
}
layer {
  name: "conv4_8/x2"
  type: "Convolution"
  bottom: "conv4_8/x2/bn"
  top: "conv4_8/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_8"
  type: "Concat"
  bottom: "concat_4_7"
  bottom: "conv4_8/x2"
  top: "concat_4_8"
}
layer {
  name: "conv4_9/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_8"
  top: "conv4_9/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv4_9/x1/scale"
  type: "Scale"
  bottom: "conv4_9/x1/bn"
  top: "conv4_9/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_9/x1"
  type: "ReLU"
  bottom: "conv4_9/x1/bn"
  top: "conv4_9/x1/bn"
}
layer {
  name: "conv4_9/x1"
  type: "Convolution"
  bottom: "conv4_9/x1/bn"
  top: "conv4_9/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_9/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_9/x1"
  top: "conv4_9/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv4_9/x2/scale"
  type: "Scale"
  bottom: "conv4_9/x2/bn"
  top: "conv4_9/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_9/x2"
  type: "ReLU"
  bottom: "conv4_9/x2/bn"
  top: "conv4_9/x2/bn"
}
layer {
  name: "conv4_9/x2"
  type: "Convolution"
  bottom: "conv4_9/x2/bn"
  top: "conv4_9/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_9"
  type: "Concat"
  bottom: "concat_4_8"
  bottom: "conv4_9/x2"
  top: "concat_4_9"
}
layer {
  name: "conv4_10/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_9"
  top: "conv4_10/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv4_10/x1/scale"
  type: "Scale"
  bottom: "conv4_10/x1/bn"
  top: "conv4_10/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_10/x1"
  type: "ReLU"
  bottom: "conv4_10/x1/bn"
  top: "conv4_10/x1/bn"
}
layer {
  name: "conv4_10/x1"
  type: "Convolution"
  bottom: "conv4_10/x1/bn"
  top: "conv4_10/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_10/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_10/x1"
  top: "conv4_10/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv4_10/x2/scale"
  type: "Scale"
  bottom: "conv4_10/x2/bn"
  top: "conv4_10/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_10/x2"
  type: "ReLU"
  bottom: "conv4_10/x2/bn"
  top: "conv4_10/x2/bn"
}
layer {
  name: "conv4_10/x2"
  type: "Convolution"
  bottom: "conv4_10/x2/bn"
  top: "conv4_10/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_10"
  type: "Concat"
  bottom: "concat_4_9"
  bottom: "conv4_10/x2"
  top: "concat_4_10"
}
layer {
  name: "conv4_11/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_10"
  top: "conv4_11/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv4_11/x1/scale"
  type: "Scale"
  bottom: "conv4_11/x1/bn"
  top: "conv4_11/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_11/x1"
  type: "ReLU"
  bottom: "conv4_11/x1/bn"
  top: "conv4_11/x1/bn"
}
layer {
  name: "conv4_11/x1"
  type: "Convolution"
  bottom: "conv4_11/x1/bn"
  top: "conv4_11/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_11/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_11/x1"
  top: "conv4_11/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv4_11/x2/scale"
  type: "Scale"
  bottom: "conv4_11/x2/bn"
  top: "conv4_11/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_11/x2"
  type: "ReLU"
  bottom: "conv4_11/x2/bn"
  top: "conv4_11/x2/bn"
}
layer {
  name: "conv4_11/x2"
  type: "Convolution"
  bottom: "conv4_11/x2/bn"
  top: "conv4_11/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_11"
  type: "Concat"
  bottom: "concat_4_10"
  bottom: "conv4_11/x2"
  top: "concat_4_11"
}
layer {
  name: "conv4_12/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_11"
  top: "conv4_12/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv4_12/x1/scale"
  type: "Scale"
  bottom: "conv4_12/x1/bn"
  top: "conv4_12/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_12/x1"
  type: "ReLU"
  bottom: "conv4_12/x1/bn"
  top: "conv4_12/x1/bn"
}
layer {
  name: "conv4_12/x1"
  type: "Convolution"
  bottom: "conv4_12/x1/bn"
  top: "conv4_12/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_12/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_12/x1"
  top: "conv4_12/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv4_12/x2/scale"
  type: "Scale"
  bottom: "conv4_12/x2/bn"
  top: "conv4_12/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_12/x2"
  type: "ReLU"
  bottom: "conv4_12/x2/bn"
  top: "conv4_12/x2/bn"
}
layer {
  name: "conv4_12/x2"
  type: "Convolution"
  bottom: "conv4_12/x2/bn"
  top: "conv4_12/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_12"
  type: "Concat"
  bottom: "concat_4_11"
  bottom: "conv4_12/x2"
  top: "concat_4_12"
}
layer {
  name: "conv4_13/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_12"
  top: "conv4_13/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv4_13/x1/scale"
  type: "Scale"
  bottom: "conv4_13/x1/bn"
  top: "conv4_13/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_13/x1"
  type: "ReLU"
  bottom: "conv4_13/x1/bn"
  top: "conv4_13/x1/bn"
}
layer {
  name: "conv4_13/x1"
  type: "Convolution"
  bottom: "conv4_13/x1/bn"
  top: "conv4_13/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_13/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_13/x1"
  top: "conv4_13/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv4_13/x2/scale"
  type: "Scale"
  bottom: "conv4_13/x2/bn"
  top: "conv4_13/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_13/x2"
  type: "ReLU"
  bottom: "conv4_13/x2/bn"
  top: "conv4_13/x2/bn"
}
layer {
  name: "conv4_13/x2"
  type: "Convolution"
  bottom: "conv4_13/x2/bn"
  top: "conv4_13/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_13"
  type: "Concat"
  bottom: "concat_4_12"
  bottom: "conv4_13/x2"
  top: "concat_4_13"
}
layer {
  name: "conv4_14/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_13"
  top: "conv4_14/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv4_14/x1/scale"
  type: "Scale"
  bottom: "conv4_14/x1/bn"
  top: "conv4_14/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_14/x1"
  type: "ReLU"
  bottom: "conv4_14/x1/bn"
  top: "conv4_14/x1/bn"
}
layer {
  name: "conv4_14/x1"
  type: "Convolution"
  bottom: "conv4_14/x1/bn"
  top: "conv4_14/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_14/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_14/x1"
  top: "conv4_14/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv4_14/x2/scale"
  type: "Scale"
  bottom: "conv4_14/x2/bn"
  top: "conv4_14/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_14/x2"
  type: "ReLU"
  bottom: "conv4_14/x2/bn"
  top: "conv4_14/x2/bn"
}
layer {
  name: "conv4_14/x2"
  type: "Convolution"
  bottom: "conv4_14/x2/bn"
  top: "conv4_14/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_14"
  type: "Concat"
  bottom: "concat_4_13"
  bottom: "conv4_14/x2"
  top: "concat_4_14"
}
layer {
  name: "conv4_15/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_14"
  top: "conv4_15/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv4_15/x1/scale"
  type: "Scale"
  bottom: "conv4_15/x1/bn"
  top: "conv4_15/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_15/x1"
  type: "ReLU"
  bottom: "conv4_15/x1/bn"
  top: "conv4_15/x1/bn"
}
layer {
  name: "conv4_15/x1"
  type: "Convolution"
  bottom: "conv4_15/x1/bn"
  top: "conv4_15/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_15/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_15/x1"
  top: "conv4_15/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv4_15/x2/scale"
  type: "Scale"
  bottom: "conv4_15/x2/bn"
  top: "conv4_15/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_15/x2"
  type: "ReLU"
  bottom: "conv4_15/x2/bn"
  top: "conv4_15/x2/bn"
}
layer {
  name: "conv4_15/x2"
  type: "Convolution"
  bottom: "conv4_15/x2/bn"
  top: "conv4_15/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_15"
  type: "Concat"
  bottom: "concat_4_14"
  bottom: "conv4_15/x2"
  top: "concat_4_15"
}
layer {
  name: "conv4_16/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_15"
  top: "conv4_16/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv4_16/x1/scale"
  type: "Scale"
  bottom: "conv4_16/x1/bn"
  top: "conv4_16/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_16/x1"
  type: "ReLU"
  bottom: "conv4_16/x1/bn"
  top: "conv4_16/x1/bn"
}
layer {
  name: "conv4_16/x1"
  type: "Convolution"
  bottom: "conv4_16/x1/bn"
  top: "conv4_16/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_16/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_16/x1"
  top: "conv4_16/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv4_16/x2/scale"
  type: "Scale"
  bottom: "conv4_16/x2/bn"
  top: "conv4_16/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_16/x2"
  type: "ReLU"
  bottom: "conv4_16/x2/bn"
  top: "conv4_16/x2/bn"
}
layer {
  name: "conv4_16/x2"
  type: "Convolution"
  bottom: "conv4_16/x2/bn"
  top: "conv4_16/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_16"
  type: "Concat"
  bottom: "concat_4_15"
  bottom: "conv4_16/x2"
  top: "concat_4_16"
}
layer {
  name: "conv4_17/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_16"
  top: "conv4_17/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv4_17/x1/scale"
  type: "Scale"
  bottom: "conv4_17/x1/bn"
  top: "conv4_17/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_17/x1"
  type: "ReLU"
  bottom: "conv4_17/x1/bn"
  top: "conv4_17/x1/bn"
}
layer {
  name: "conv4_17/x1"
  type: "Convolution"
  bottom: "conv4_17/x1/bn"
  top: "conv4_17/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_17/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_17/x1"
  top: "conv4_17/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv4_17/x2/scale"
  type: "Scale"
  bottom: "conv4_17/x2/bn"
  top: "conv4_17/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_17/x2"
  type: "ReLU"
  bottom: "conv4_17/x2/bn"
  top: "conv4_17/x2/bn"
}
layer {
  name: "conv4_17/x2"
  type: "Convolution"
  bottom: "conv4_17/x2/bn"
  top: "conv4_17/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_17"
  type: "Concat"
  bottom: "concat_4_16"
  bottom: "conv4_17/x2"
  top: "concat_4_17"
}
layer {
  name: "conv4_18/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_17"
  top: "conv4_18/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv4_18/x1/scale"
  type: "Scale"
  bottom: "conv4_18/x1/bn"
  top: "conv4_18/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_18/x1"
  type: "ReLU"
  bottom: "conv4_18/x1/bn"
  top: "conv4_18/x1/bn"
}
layer {
  name: "conv4_18/x1"
  type: "Convolution"
  bottom: "conv4_18/x1/bn"
  top: "conv4_18/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_18/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_18/x1"
  top: "conv4_18/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv4_18/x2/scale"
  type: "Scale"
  bottom: "conv4_18/x2/bn"
  top: "conv4_18/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_18/x2"
  type: "ReLU"
  bottom: "conv4_18/x2/bn"
  top: "conv4_18/x2/bn"
}
layer {
  name: "conv4_18/x2"
  type: "Convolution"
  bottom: "conv4_18/x2/bn"
  top: "conv4_18/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_18"
  type: "Concat"
  bottom: "concat_4_17"
  bottom: "conv4_18/x2"
  top: "concat_4_18"
}
layer {
  name: "conv4_19/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_18"
  top: "conv4_19/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv4_19/x1/scale"
  type: "Scale"
  bottom: "conv4_19/x1/bn"
  top: "conv4_19/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_19/x1"
  type: "ReLU"
  bottom: "conv4_19/x1/bn"
  top: "conv4_19/x1/bn"
}
layer {
  name: "conv4_19/x1"
  type: "Convolution"
  bottom: "conv4_19/x1/bn"
  top: "conv4_19/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_19/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_19/x1"
  top: "conv4_19/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv4_19/x2/scale"
  type: "Scale"
  bottom: "conv4_19/x2/bn"
  top: "conv4_19/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_19/x2"
  type: "ReLU"
  bottom: "conv4_19/x2/bn"
  top: "conv4_19/x2/bn"
}
layer {
  name: "conv4_19/x2"
  type: "Convolution"
  bottom: "conv4_19/x2/bn"
  top: "conv4_19/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_19"
  type: "Concat"
  bottom: "concat_4_18"
  bottom: "conv4_19/x2"
  top: "concat_4_19"
}
layer {
  name: "conv4_20/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_19"
  top: "conv4_20/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv4_20/x1/scale"
  type: "Scale"
  bottom: "conv4_20/x1/bn"
  top: "conv4_20/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_20/x1"
  type: "ReLU"
  bottom: "conv4_20/x1/bn"
  top: "conv4_20/x1/bn"
}
layer {
  name: "conv4_20/x1"
  type: "Convolution"
  bottom: "conv4_20/x1/bn"
  top: "conv4_20/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_20/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_20/x1"
  top: "conv4_20/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv4_20/x2/scale"
  type: "Scale"
  bottom: "conv4_20/x2/bn"
  top: "conv4_20/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_20/x2"
  type: "ReLU"
  bottom: "conv4_20/x2/bn"
  top: "conv4_20/x2/bn"
}
layer {
  name: "conv4_20/x2"
  type: "Convolution"
  bottom: "conv4_20/x2/bn"
  top: "conv4_20/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_20"
  type: "Concat"
  bottom: "concat_4_19"
  bottom: "conv4_20/x2"
  top: "concat_4_20"
}
layer {
  name: "conv4_21/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_20"
  top: "conv4_21/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv4_21/x1/scale"
  type: "Scale"
  bottom: "conv4_21/x1/bn"
  top: "conv4_21/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_21/x1"
  type: "ReLU"
  bottom: "conv4_21/x1/bn"
  top: "conv4_21/x1/bn"
}
layer {
  name: "conv4_21/x1"
  type: "Convolution"
  bottom: "conv4_21/x1/bn"
  top: "conv4_21/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_21/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_21/x1"
  top: "conv4_21/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv4_21/x2/scale"
  type: "Scale"
  bottom: "conv4_21/x2/bn"
  top: "conv4_21/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_21/x2"
  type: "ReLU"
  bottom: "conv4_21/x2/bn"
  top: "conv4_21/x2/bn"
}
layer {
  name: "conv4_21/x2"
  type: "Convolution"
  bottom: "conv4_21/x2/bn"
  top: "conv4_21/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_21"
  type: "Concat"
  bottom: "concat_4_20"
  bottom: "conv4_21/x2"
  top: "concat_4_21"
}
layer {
  name: "conv4_22/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_21"
  top: "conv4_22/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv4_22/x1/scale"
  type: "Scale"
  bottom: "conv4_22/x1/bn"
  top: "conv4_22/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_22/x1"
  type: "ReLU"
  bottom: "conv4_22/x1/bn"
  top: "conv4_22/x1/bn"
}
layer {
  name: "conv4_22/x1"
  type: "Convolution"
  bottom: "conv4_22/x1/bn"
  top: "conv4_22/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_22/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_22/x1"
  top: "conv4_22/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv4_22/x2/scale"
  type: "Scale"
  bottom: "conv4_22/x2/bn"
  top: "conv4_22/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_22/x2"
  type: "ReLU"
  bottom: "conv4_22/x2/bn"
  top: "conv4_22/x2/bn"
}
layer {
  name: "conv4_22/x2"
  type: "Convolution"
  bottom: "conv4_22/x2/bn"
  top: "conv4_22/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_22"
  type: "Concat"
  bottom: "concat_4_21"
  bottom: "conv4_22/x2"
  top: "concat_4_22"
}
layer {
  name: "conv4_23/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_22"
  top: "conv4_23/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv4_23/x1/scale"
  type: "Scale"
  bottom: "conv4_23/x1/bn"
  top: "conv4_23/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_23/x1"
  type: "ReLU"
  bottom: "conv4_23/x1/bn"
  top: "conv4_23/x1/bn"
}
layer {
  name: "conv4_23/x1"
  type: "Convolution"
  bottom: "conv4_23/x1/bn"
  top: "conv4_23/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_23/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_23/x1"
  top: "conv4_23/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv4_23/x2/scale"
  type: "Scale"
  bottom: "conv4_23/x2/bn"
  top: "conv4_23/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_23/x2"
  type: "ReLU"
  bottom: "conv4_23/x2/bn"
  top: "conv4_23/x2/bn"
}
layer {
  name: "conv4_23/x2"
  type: "Convolution"
  bottom: "conv4_23/x2/bn"
  top: "conv4_23/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_23"
  type: "Concat"
  bottom: "concat_4_22"
  bottom: "conv4_23/x2"
  top: "concat_4_23"
}
layer {
  name: "conv4_24/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_23"
  top: "conv4_24/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv4_24/x1/scale"
  type: "Scale"
  bottom: "conv4_24/x1/bn"
  top: "conv4_24/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_24/x1"
  type: "ReLU"
  bottom: "conv4_24/x1/bn"
  top: "conv4_24/x1/bn"
}
layer {
  name: "conv4_24/x1"
  type: "Convolution"
  bottom: "conv4_24/x1/bn"
  top: "conv4_24/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_24/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_24/x1"
  top: "conv4_24/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv4_24/x2/scale"
  type: "Scale"
  bottom: "conv4_24/x2/bn"
  top: "conv4_24/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_24/x2"
  type: "ReLU"
  bottom: "conv4_24/x2/bn"
  top: "conv4_24/x2/bn"
}
layer {
  name: "conv4_24/x2"
  type: "Convolution"
  bottom: "conv4_24/x2/bn"
  top: "conv4_24/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_24"
  type: "Concat"
  bottom: "concat_4_23"
  bottom: "conv4_24/x2"
  top: "concat_4_24"
}
layer {
  name: "conv4_blk/bn"
  type: "BatchNorm"
  bottom: "concat_4_24"
  top: "conv4_blk/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv4_blk/scale"
  type: "Scale"
  bottom: "conv4_blk/bn"
  top: "conv4_blk/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_blk"
  type: "ReLU"
  bottom: "conv4_blk/bn"
  top: "conv4_blk/bn"
}
layer {
  name: "conv4_blk"
  type: "Convolution"
  bottom: "conv4_blk/bn"
  top: "conv4_blk"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 512
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4_blk"
  top: "pool4"
  pooling_param {
    pool: AVE
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv5_1/x1/bn"
  type: "BatchNorm"
  bottom: "pool4"
  top: "conv5_1/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv5_1/x1/scale"
  type: "Scale"
  bottom: "conv5_1/x1/bn"
  top: "conv5_1/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_1/x1"
  type: "ReLU"
  bottom: "conv5_1/x1/bn"
  top: "conv5_1/x1/bn"
}
layer {
  name: "conv5_1/x1"
  type: "Convolution"
  bottom: "conv5_1/x1/bn"
  top: "conv5_1/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv5_1/x2/bn"
  type: "BatchNorm"
  bottom: "conv5_1/x1"
  top: "conv5_1/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv5_1/x2/scale"
  type: "Scale"
  bottom: "conv5_1/x2/bn"
  top: "conv5_1/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_1/x2"
  type: "ReLU"
  bottom: "conv5_1/x2/bn"
  top: "conv5_1/x2/bn"
}
layer {
  name: "conv5_1/x2"
  type: "Convolution"
  bottom: "conv5_1/x2/bn"
  top: "conv5_1/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_5_1"
  type: "Concat"
  bottom: "pool4"
  bottom: "conv5_1/x2"
  top: "concat_5_1"
}
layer {
  name: "conv5_2/x1/bn"
  type: "BatchNorm"
  bottom: "concat_5_1"
  top: "conv5_2/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv5_2/x1/scale"
  type: "Scale"
  bottom: "conv5_2/x1/bn"
  top: "conv5_2/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_2/x1"
  type: "ReLU"
  bottom: "conv5_2/x1/bn"
  top: "conv5_2/x1/bn"
}
layer {
  name: "conv5_2/x1"
  type: "Convolution"
  bottom: "conv5_2/x1/bn"
  top: "conv5_2/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv5_2/x2/bn"
  type: "BatchNorm"
  bottom: "conv5_2/x1"
  top: "conv5_2/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv5_2/x2/scale"
  type: "Scale"
  bottom: "conv5_2/x2/bn"
  top: "conv5_2/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_2/x2"
  type: "ReLU"
  bottom: "conv5_2/x2/bn"
  top: "conv5_2/x2/bn"
}
layer {
  name: "conv5_2/x2"
  type: "Convolution"
  bottom: "conv5_2/x2/bn"
  top: "conv5_2/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_5_2"
  type: "Concat"
  bottom: "concat_5_1"
  bottom: "conv5_2/x2"
  top: "concat_5_2"
}
layer {
  name: "conv5_3/x1/bn"
  type: "BatchNorm"
  bottom: "concat_5_2"
  top: "conv5_3/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv5_3/x1/scale"
  type: "Scale"
  bottom: "conv5_3/x1/bn"
  top: "conv5_3/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_3/x1"
  type: "ReLU"
  bottom: "conv5_3/x1/bn"
  top: "conv5_3/x1/bn"
}
layer {
  name: "conv5_3/x1"
  type: "Convolution"
  bottom: "conv5_3/x1/bn"
  top: "conv5_3/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv5_3/x2/bn"
  type: "BatchNorm"
  bottom: "conv5_3/x1"
  top: "conv5_3/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv5_3/x2/scale"
  type: "Scale"
  bottom: "conv5_3/x2/bn"
  top: "conv5_3/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_3/x2"
  type: "ReLU"
  bottom: "conv5_3/x2/bn"
  top: "conv5_3/x2/bn"
}
layer {
  name: "conv5_3/x2"
  type: "Convolution"
  bottom: "conv5_3/x2/bn"
  top: "conv5_3/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_5_3"
  type: "Concat"
  bottom: "concat_5_2"
  bottom: "conv5_3/x2"
  top: "concat_5_3"
}
layer {
  name: "conv5_4/x1/bn"
  type: "BatchNorm"
  bottom: "concat_5_3"
  top: "conv5_4/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv5_4/x1/scale"
  type: "Scale"
  bottom: "conv5_4/x1/bn"
  top: "conv5_4/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_4/x1"
  type: "ReLU"
  bottom: "conv5_4/x1/bn"
  top: "conv5_4/x1/bn"
}
layer {
  name: "conv5_4/x1"
  type: "Convolution"
  bottom: "conv5_4/x1/bn"
  top: "conv5_4/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv5_4/x2/bn"
  type: "BatchNorm"
  bottom: "conv5_4/x1"
  top: "conv5_4/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv5_4/x2/scale"
  type: "Scale"
  bottom: "conv5_4/x2/bn"
  top: "conv5_4/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_4/x2"
  type: "ReLU"
  bottom: "conv5_4/x2/bn"
  top: "conv5_4/x2/bn"
}
layer {
  name: "conv5_4/x2"
  type: "Convolution"
  bottom: "conv5_4/x2/bn"
  top: "conv5_4/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_5_4"
  type: "Concat"
  bottom: "concat_5_3"
  bottom: "conv5_4/x2"
  top: "concat_5_4"
}
layer {
  name: "conv5_5/x1/bn"
  type: "BatchNorm"
  bottom: "concat_5_4"
  top: "conv5_5/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv5_5/x1/scale"
  type: "Scale"
  bottom: "conv5_5/x1/bn"
  top: "conv5_5/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_5/x1"
  type: "ReLU"
  bottom: "conv5_5/x1/bn"
  top: "conv5_5/x1/bn"
}
layer {
  name: "conv5_5/x1"
  type: "Convolution"
  bottom: "conv5_5/x1/bn"
  top: "conv5_5/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv5_5/x2/bn"
  type: "BatchNorm"
  bottom: "conv5_5/x1"
  top: "conv5_5/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv5_5/x2/scale"
  type: "Scale"
  bottom: "conv5_5/x2/bn"
  top: "conv5_5/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_5/x2"
  type: "ReLU"
  bottom: "conv5_5/x2/bn"
  top: "conv5_5/x2/bn"
}
layer {
  name: "conv5_5/x2"
  type: "Convolution"
  bottom: "conv5_5/x2/bn"
  top: "conv5_5/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_5_5"
  type: "Concat"
  bottom: "concat_5_4"
  bottom: "conv5_5/x2"
  top: "concat_5_5"
}
layer {
  name: "conv5_6/x1/bn"
  type: "BatchNorm"
  bottom: "concat_5_5"
  top: "conv5_6/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv5_6/x1/scale"
  type: "Scale"
  bottom: "conv5_6/x1/bn"
  top: "conv5_6/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_6/x1"
  type: "ReLU"
  bottom: "conv5_6/x1/bn"
  top: "conv5_6/x1/bn"
}
layer {
  name: "conv5_6/x1"
  type: "Convolution"
  bottom: "conv5_6/x1/bn"
  top: "conv5_6/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv5_6/x2/bn"
  type: "BatchNorm"
  bottom: "conv5_6/x1"
  top: "conv5_6/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv5_6/x2/scale"
  type: "Scale"
  bottom: "conv5_6/x2/bn"
  top: "conv5_6/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_6/x2"
  type: "ReLU"
  bottom: "conv5_6/x2/bn"
  top: "conv5_6/x2/bn"
}
layer {
  name: "conv5_6/x2"
  type: "Convolution"
  bottom: "conv5_6/x2/bn"
  top: "conv5_6/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_5_6"
  type: "Concat"
  bottom: "concat_5_5"
  bottom: "conv5_6/x2"
  top: "concat_5_6"
}
layer {
  name: "conv5_7/x1/bn"
  type: "BatchNorm"
  bottom: "concat_5_6"
  top: "conv5_7/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv5_7/x1/scale"
  type: "Scale"
  bottom: "conv5_7/x1/bn"
  top: "conv5_7/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_7/x1"
  type: "ReLU"
  bottom: "conv5_7/x1/bn"
  top: "conv5_7/x1/bn"
}
layer {
  name: "conv5_7/x1"
  type: "Convolution"
  bottom: "conv5_7/x1/bn"
  top: "conv5_7/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv5_7/x2/bn"
  type: "BatchNorm"
  bottom: "conv5_7/x1"
  top: "conv5_7/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv5_7/x2/scale"
  type: "Scale"
  bottom: "conv5_7/x2/bn"
  top: "conv5_7/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_7/x2"
  type: "ReLU"
  bottom: "conv5_7/x2/bn"
  top: "conv5_7/x2/bn"
}
layer {
  name: "conv5_7/x2"
  type: "Convolution"
  bottom: "conv5_7/x2/bn"
  top: "conv5_7/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_5_7"
  type: "Concat"
  bottom: "concat_5_6"
  bottom: "conv5_7/x2"
  top: "concat_5_7"
}
layer {
  name: "conv5_8/x1/bn"
  type: "BatchNorm"
  bottom: "concat_5_7"
  top: "conv5_8/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv5_8/x1/scale"
  type: "Scale"
  bottom: "conv5_8/x1/bn"
  top: "conv5_8/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_8/x1"
  type: "ReLU"
  bottom: "conv5_8/x1/bn"
  top: "conv5_8/x1/bn"
}
layer {
  name: "conv5_8/x1"
  type: "Convolution"
  bottom: "conv5_8/x1/bn"
  top: "conv5_8/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv5_8/x2/bn"
  type: "BatchNorm"
  bottom: "conv5_8/x1"
  top: "conv5_8/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv5_8/x2/scale"
  type: "Scale"
  bottom: "conv5_8/x2/bn"
  top: "conv5_8/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_8/x2"
  type: "ReLU"
  bottom: "conv5_8/x2/bn"
  top: "conv5_8/x2/bn"
}
layer {
  name: "conv5_8/x2"
  type: "Convolution"
  bottom: "conv5_8/x2/bn"
  top: "conv5_8/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_5_8"
  type: "Concat"
  bottom: "concat_5_7"
  bottom: "conv5_8/x2"
  top: "concat_5_8"
}
layer {
  name: "conv5_9/x1/bn"
  type: "BatchNorm"
  bottom: "concat_5_8"
  top: "conv5_9/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv5_9/x1/scale"
  type: "Scale"
  bottom: "conv5_9/x1/bn"
  top: "conv5_9/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_9/x1"
  type: "ReLU"
  bottom: "conv5_9/x1/bn"
  top: "conv5_9/x1/bn"
}
layer {
  name: "conv5_9/x1"
  type: "Convolution"
  bottom: "conv5_9/x1/bn"
  top: "conv5_9/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv5_9/x2/bn"
  type: "BatchNorm"
  bottom: "conv5_9/x1"
  top: "conv5_9/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv5_9/x2/scale"
  type: "Scale"
  bottom: "conv5_9/x2/bn"
  top: "conv5_9/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_9/x2"
  type: "ReLU"
  bottom: "conv5_9/x2/bn"
  top: "conv5_9/x2/bn"
}
layer {
  name: "conv5_9/x2"
  type: "Convolution"
  bottom: "conv5_9/x2/bn"
  top: "conv5_9/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_5_9"
  type: "Concat"
  bottom: "concat_5_8"
  bottom: "conv5_9/x2"
  top: "concat_5_9"
}
layer {
  name: "conv5_10/x1/bn"
  type: "BatchNorm"
  bottom: "concat_5_9"
  top: "conv5_10/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv5_10/x1/scale"
  type: "Scale"
  bottom: "conv5_10/x1/bn"
  top: "conv5_10/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_10/x1"
  type: "ReLU"
  bottom: "conv5_10/x1/bn"
  top: "conv5_10/x1/bn"
}
layer {
  name: "conv5_10/x1"
  type: "Convolution"
  bottom: "conv5_10/x1/bn"
  top: "conv5_10/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv5_10/x2/bn"
  type: "BatchNorm"
  bottom: "conv5_10/x1"
  top: "conv5_10/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv5_10/x2/scale"
  type: "Scale"
  bottom: "conv5_10/x2/bn"
  top: "conv5_10/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_10/x2"
  type: "ReLU"
  bottom: "conv5_10/x2/bn"
  top: "conv5_10/x2/bn"
}
layer {
  name: "conv5_10/x2"
  type: "Convolution"
  bottom: "conv5_10/x2/bn"
  top: "conv5_10/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_5_10"
  type: "Concat"
  bottom: "concat_5_9"
  bottom: "conv5_10/x2"
  top: "concat_5_10"
}
layer {
  name: "conv5_11/x1/bn"
  type: "BatchNorm"
  bottom: "concat_5_10"
  top: "conv5_11/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv5_11/x1/scale"
  type: "Scale"
  bottom: "conv5_11/x1/bn"
  top: "conv5_11/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_11/x1"
  type: "ReLU"
  bottom: "conv5_11/x1/bn"
  top: "conv5_11/x1/bn"
}
layer {
  name: "conv5_11/x1"
  type: "Convolution"
  bottom: "conv5_11/x1/bn"
  top: "conv5_11/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv5_11/x2/bn"
  type: "BatchNorm"
  bottom: "conv5_11/x1"
  top: "conv5_11/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv5_11/x2/scale"
  type: "Scale"
  bottom: "conv5_11/x2/bn"
  top: "conv5_11/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_11/x2"
  type: "ReLU"
  bottom: "conv5_11/x2/bn"
  top: "conv5_11/x2/bn"
}
layer {
  name: "conv5_11/x2"
  type: "Convolution"
  bottom: "conv5_11/x2/bn"
  top: "conv5_11/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_5_11"
  type: "Concat"
  bottom: "concat_5_10"
  bottom: "conv5_11/x2"
  top: "concat_5_11"
}
layer {
  name: "conv5_12/x1/bn"
  type: "BatchNorm"
  bottom: "concat_5_11"
  top: "conv5_12/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv5_12/x1/scale"
  type: "Scale"
  bottom: "conv5_12/x1/bn"
  top: "conv5_12/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_12/x1"
  type: "ReLU"
  bottom: "conv5_12/x1/bn"
  top: "conv5_12/x1/bn"
}
layer {
  name: "conv5_12/x1"
  type: "Convolution"
  bottom: "conv5_12/x1/bn"
  top: "conv5_12/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv5_12/x2/bn"
  type: "BatchNorm"
  bottom: "conv5_12/x1"
  top: "conv5_12/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv5_12/x2/scale"
  type: "Scale"
  bottom: "conv5_12/x2/bn"
  top: "conv5_12/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_12/x2"
  type: "ReLU"
  bottom: "conv5_12/x2/bn"
  top: "conv5_12/x2/bn"
}
layer {
  name: "conv5_12/x2"
  type: "Convolution"
  bottom: "conv5_12/x2/bn"
  top: "conv5_12/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_5_12"
  type: "Concat"
  bottom: "concat_5_11"
  bottom: "conv5_12/x2"
  top: "concat_5_12"
}
layer {
  name: "conv5_13/x1/bn"
  type: "BatchNorm"
  bottom: "concat_5_12"
  top: "conv5_13/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv5_13/x1/scale"
  type: "Scale"
  bottom: "conv5_13/x1/bn"
  top: "conv5_13/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_13/x1"
  type: "ReLU"
  bottom: "conv5_13/x1/bn"
  top: "conv5_13/x1/bn"
}
layer {
  name: "conv5_13/x1"
  type: "Convolution"
  bottom: "conv5_13/x1/bn"
  top: "conv5_13/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv5_13/x2/bn"
  type: "BatchNorm"
  bottom: "conv5_13/x1"
  top: "conv5_13/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv5_13/x2/scale"
  type: "Scale"
  bottom: "conv5_13/x2/bn"
  top: "conv5_13/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_13/x2"
  type: "ReLU"
  bottom: "conv5_13/x2/bn"
  top: "conv5_13/x2/bn"
}
layer {
  name: "conv5_13/x2"
  type: "Convolution"
  bottom: "conv5_13/x2/bn"
  top: "conv5_13/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_5_13"
  type: "Concat"
  bottom: "concat_5_12"
  bottom: "conv5_13/x2"
  top: "concat_5_13"
}
layer {
  name: "conv5_14/x1/bn"
  type: "BatchNorm"
  bottom: "concat_5_13"
  top: "conv5_14/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv5_14/x1/scale"
  type: "Scale"
  bottom: "conv5_14/x1/bn"
  top: "conv5_14/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_14/x1"
  type: "ReLU"
  bottom: "conv5_14/x1/bn"
  top: "conv5_14/x1/bn"
}
layer {
  name: "conv5_14/x1"
  type: "Convolution"
  bottom: "conv5_14/x1/bn"
  top: "conv5_14/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv5_14/x2/bn"
  type: "BatchNorm"
  bottom: "conv5_14/x1"
  top: "conv5_14/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv5_14/x2/scale"
  type: "Scale"
  bottom: "conv5_14/x2/bn"
  top: "conv5_14/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_14/x2"
  type: "ReLU"
  bottom: "conv5_14/x2/bn"
  top: "conv5_14/x2/bn"
}
layer {
  name: "conv5_14/x2"
  type: "Convolution"
  bottom: "conv5_14/x2/bn"
  top: "conv5_14/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_5_14"
  type: "Concat"
  bottom: "concat_5_13"
  bottom: "conv5_14/x2"
  top: "concat_5_14"
}
layer {
  name: "conv5_15/x1/bn"
  type: "BatchNorm"
  bottom: "concat_5_14"
  top: "conv5_15/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv5_15/x1/scale"
  type: "Scale"
  bottom: "conv5_15/x1/bn"
  top: "conv5_15/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_15/x1"
  type: "ReLU"
  bottom: "conv5_15/x1/bn"
  top: "conv5_15/x1/bn"
}
layer {
  name: "conv5_15/x1"
  type: "Convolution"
  bottom: "conv5_15/x1/bn"
  top: "conv5_15/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv5_15/x2/bn"
  type: "BatchNorm"
  bottom: "conv5_15/x1"
  top: "conv5_15/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv5_15/x2/scale"
  type: "Scale"
  bottom: "conv5_15/x2/bn"
  top: "conv5_15/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_15/x2"
  type: "ReLU"
  bottom: "conv5_15/x2/bn"
  top: "conv5_15/x2/bn"
}
layer {
  name: "conv5_15/x2"
  type: "Convolution"
  bottom: "conv5_15/x2/bn"
  top: "conv5_15/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_5_15"
  type: "Concat"
  bottom: "concat_5_14"
  bottom: "conv5_15/x2"
  top: "concat_5_15"
}
layer {
  name: "conv5_16/x1/bn"
  type: "BatchNorm"
  bottom: "concat_5_15"
  top: "conv5_16/x1/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv5_16/x1/scale"
  type: "Scale"
  bottom: "conv5_16/x1/bn"
  top: "conv5_16/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_16/x1"
  type: "ReLU"
  bottom: "conv5_16/x1/bn"
  top: "conv5_16/x1/bn"
}
layer {
  name: "conv5_16/x1"
  type: "Convolution"
  bottom: "conv5_16/x1/bn"
  top: "conv5_16/x1"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv5_16/x2/bn"
  type: "BatchNorm"
  bottom: "conv5_16/x1"
  top: "conv5_16/x2/bn"
  batch_norm_param{
    use_global_stats : true
    eps: 1e-5
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "conv5_16/x2/scale"
  type: "Scale"
  bottom: "conv5_16/x2/bn"
  top: "conv5_16/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_16/x2"
  type: "ReLU"
  bottom: "conv5_16/x2/bn"
  top: "conv5_16/x2/bn"
}
layer {
  name: "conv5_16/x2"
  type: "Convolution"
  bottom: "conv5_16/x2/bn"
  top: "conv5_16/x2"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_5_16"
  type: "Concat"
  bottom: "concat_5_15"
  bottom: "conv5_16/x2"
  top: "concat_5_16"
}
# layer {
#   name: "conv5_blk/bn"
#   type: "BatchNorm"
#   bottom: "concat_5_16"
#   top: "conv5_blk/bn"
#   batch_norm_param{
#     use_global_stats : true
#     eps: 1e-5
#   }
#   param {lr_mult: 0 decay_mult: 0}
#   param {lr_mult: 0 decay_mult: 0}
#   param {lr_mult: 0 decay_mult: 0}
# }
# layer {
#   name: "conv5_blk/scale"
#   type: "Scale"
#   bottom: "conv5_blk/bn"
#   top: "conv5_blk/bn"
#   scale_param {
#     bias_term: true
#   }
# }
# layer {
#   name: "relu5_blk"
#   type: "ReLU"
#   bottom: "conv5_blk/bn"
#   top: "conv5_blk/bn"
# }
# layer {
#   name: "pool5"
#   type: "Pooling"
#   bottom: "conv5_blk/bn"
#   top: "pool5"
#   pooling_param {
#     pool: AVE
#     global_pooling: true
#   }
# }
# layer {
#   name: "fc6"
#   type: "Convolution"
#   bottom: "pool5"
#   top: "fc6"
#   convolution_param {
#     num_output: 1000
#     kernel_size: 1
#   }
# }




############################################################################################
# Decoding part of the main network
############################################################################################
# 1/32
# Squeeze and Excitation for concat_5_16
layer {
  name: "pool_SE_1/32"
  type: "Pooling"
  bottom: "concat_5_16"
  top: "pool_SE_1/32"
  pooling_param {
    engine: CAFFE
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "conv_SE_1/32_1"
  type: "Convolution"
  bottom: "pool_SE_1/32"
  top: "conv_SE_1/32_1"
  param { lr_mult: 1 decay_mult: 1 }
  param { lr_mult: 2 decay_mult: 0 }
  convolution_param {
    num_output: 64
    kernel_size: 1
    stride: 1
    weight_filler { type: "gaussian" std: 0.01 } 
  }
}
layer {
  name: "relu_SE_1/32_1"
  type: "ReLU"
  bottom: "conv_SE_1/32_1"
  top: "conv_SE_1/32_1"
}
layer {
  name: "conv_SE_1/32_2"
  type: "Convolution"
  bottom: "conv_SE_1/32_1"
  top: "conv_SE_1/32_2"
  param { lr_mult: 1 decay_mult: 1 }
  param { lr_mult: 2 decay_mult: 0 }
  convolution_param {
    num_output: 1024
    kernel_size: 1
    stride: 1
    weight_filler { type: "gaussian" std: 0.01 } 
  }
}
layer {
  name: "sigm_SE_1/32"
  type: "Sigmoid"
  bottom: "conv_SE_1/32_2"
  top: "sigm_SE_1/32"
}
layer {
  name: "reshape_SE_1/32"
  type: "Reshape"
  bottom: "sigm_SE_1/32"
  top: "reshape_SE_1/32"
  reshape_param {
    shape {
      dim: 0
      dim: 0
    }
  }
}
layer {
  name: "scale_SE_1/32"
  type: "Scale"
  bottom: "concat_5_16"
  bottom: "reshape_SE_1/32"
  top: "scale_SE_1/32"
  scale_param {
    axis: 0
    bias_term: false
  }
}


layer {
  bottom: "scale_SE_1/32"
  top: "conv_1/32_1d"
  name: "conv_1/32_1d"
  type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    bias_term: false
    weight_filler { type: "gaussian" std: 0.01 } 
  }
}
layer {
  bottom: "conv_1/32_1d"
  top: "conv_1/32_1d"
  name: "prelu_1/32_1d"
  type: "PReLU"
}
layer {
  name: "bn_1/32_1d"
  type: "BatchNorm"
  bottom: "conv_1/32_1d"
  top: "conv_1/32_1d"
  batch_norm_param{
    use_global_stats : true
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  bottom: "conv_1/32_1d"
  top: "conv_1/32_2d"
  name: "conv_1/32_2d"
  type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    bias_term: false
    weight_filler { type: "gaussian" std: 0.01 } 
  }
}
layer {
  bottom: "conv_1/32_2d"
  top: "conv_1/32_2d"
  name: "prelu_1/32_2d"
  type: "PReLU"
}
layer {
  name: "bn_1/32_2d"
  type: "BatchNorm"
  bottom: "conv_1/32_2d"
  top: "conv_1/32_2d"
  batch_norm_param{
    use_global_stats : true
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  bottom: "conv_1/32_2d"
  top: "conv_1/32_3d"
  name: "conv_1/32_3d"
  type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 256
    kernel_size: 1
    bias_term: false
    weight_filler { type: "gaussian" std: 0.01 } 
  }
}
layer {
  bottom: "conv_1/32_3d"
  top: "conv_1/32_3d"
  name: "prelu_1/32_3d"
  type: "PReLU"
}
layer {
  name: "bn_1/32_3d"
  type: "BatchNorm"
  bottom: "conv_1/32_3d"
  top: "conv_1/32_3d"
  batch_norm_param{
    use_global_stats : true
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
# 1/16
layer {
  bottom: "conv_1/32_3d"
  top: "deconv_1/16d"
  name: "deconv_1/16d"
  type: "Deconvolution"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 256
    kernel_size: 2
    stride: 2
    bias_term: false
    weight_filler { type: "gaussian" std: 0.01 } 
  }
}
layer {
  bottom: "deconv_1/16d"
  top: "deconv_1/16d"
  name: "prelu_1/16d"
  type: "PReLU"
}
layer {
  name: "bn_1/16d"
  type: "BatchNorm"
  bottom: "deconv_1/16d"
  top: "deconv_1/16d"
  batch_norm_param{
    use_global_stats : true
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}

# Squeeze and Excitation for concat_4_24
layer {
  name: "pool_SE_1/16"
  type: "Pooling"
  bottom: "concat_4_24"
  top: "pool_SE_1/16"
  pooling_param {
    engine: CAFFE
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "conv_SE_1/16_1"
  type: "Convolution"
  bottom: "pool_SE_1/16"
  top: "conv_SE_1/16_1"
  param { lr_mult: 1 decay_mult: 1 }
  param { lr_mult: 2 decay_mult: 0 }
  convolution_param {
    num_output: 64
    kernel_size: 1
    stride: 1
    weight_filler { type: "gaussian" std: 0.01 } 
  }
}
layer {
  name: "relu_SE_1/16_1"
  type: "ReLU"
  bottom: "conv_SE_1/16_1"
  top: "conv_SE_1/16_1"
}
layer {
  name: "conv_SE_1/16_2"
  type: "Convolution"
  bottom: "conv_SE_1/16_1"
  top: "conv_SE_1/16_2"
  param { lr_mult: 1 decay_mult: 1 }
  param { lr_mult: 2 decay_mult: 0 }
  convolution_param {
    num_output: 1024
    kernel_size: 1
    stride: 1
    weight_filler { type: "gaussian" std: 0.01 } 
  }
}
layer {
  name: "sigm_SE_1/16"
  type: "Sigmoid"
  bottom: "conv_SE_1/16_2"
  top: "sigm_SE_1/16"
}
layer {
  name: "reshape_SE_1/16"
  type: "Reshape"
  bottom: "sigm_SE_1/16"
  top: "reshape_SE_1/16"
  reshape_param {
    shape {
      dim: 0
      dim: 0
    }
  }
}
layer {
  name: "scale_SE_1/16"
  type: "Scale"
  bottom: "concat_4_24"
  bottom: "reshape_SE_1/16"
  top: "scale_SE_1/16"
  scale_param {
    axis: 0
    bias_term: false
  }
}
layer {
  name: "conv_SE_1/16"
  type: "Convolution"
  bottom: "scale_SE_1/16"
  top: "conv_SE_1/16"
  param { lr_mult: 1 decay_mult: 1 }
  param { lr_mult: 2 decay_mult: 0 }
  convolution_param {
    num_output: 256
    kernel_size: 1
    stride: 1
    weight_filler { type: "gaussian" std: 0.01 } 
  }
}
layer {
  bottom: "conv_SE_1/16"
  top: "conv_SE_1/16"
  name: "prelu_SE_1/16"
  type: "PReLU"
}
layer {
  name: "bn_SE_1/16"
  type: "BatchNorm"
  bottom: "conv_SE_1/16"
  top: "conv_SE_1/16"
  batch_norm_param{
    use_global_stats : true
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}



layer {
  name: "concat_1/16d"
  type: "Concat"
  bottom: "deconv_1/16d"
  bottom: "conv_SE_1/16"
  top: "concat_1/16d"
  concat_param {
    concat_dim: 1
  }
}
layer {
  bottom: "concat_1/16d"
  top: "conv_1/16_1d"
  name: "conv_1/16_1d"
  type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    bias_term: false
    weight_filler { type: "gaussian" std: 0.01 } 
  }
}
layer {
  bottom: "conv_1/16_1d"
  top: "conv_1/16_1d"
  name: "prelu_1/16_1d"
  type: "PReLU"
}
layer {
  name: "bn_1/16_1d"
  type: "BatchNorm"
  bottom: "conv_1/16_1d"
  top: "conv_1/16_1d"
  batch_norm_param{
    use_global_stats : true
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  bottom: "conv_1/16_1d"
  top: "conv_1/16_2d"
  name: "conv_1/16_2d"
  type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    bias_term: false
    weight_filler { type: "gaussian" std: 0.01 } 
  }
}
layer {
  bottom: "conv_1/16_2d"
  top: "conv_1/16_2d"
  name: "prelu_1/16_2d"
  type: "PReLU"
}
layer {
  name: "bn_1/16_2d"
  type: "BatchNorm"
  bottom: "conv_1/16_2d"
  top: "conv_1/16_2d"
  batch_norm_param{
    use_global_stats : true
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  bottom: "conv_1/16_2d"
  top: "conv_1/16_3d"
  name: "conv_1/16_3d"
  type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    kernel_size: 1
    bias_term: false
    weight_filler { type: "gaussian" std: 0.01 } 
  }
}
layer {
  bottom: "conv_1/16_3d"
  top: "conv_1/16_3d"
  name: "prelu_1/16_3d"
  type: "PReLU"
}
layer {
  name: "bn_1/16_3d"
  type: "BatchNorm"
  bottom: "conv_1/16_3d"
  top: "conv_1/16_3d"
  batch_norm_param{
    use_global_stats : true
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
# 1/8
layer {
  bottom: "conv_1/16_3d"
  top: "deconv_1/8d"
  name: "deconv_1/8d"
  type: "Deconvolution"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    kernel_size: 2
    stride: 2
    bias_term: false
    weight_filler { type: "gaussian" std: 0.01 } 
  }
}
layer {
  bottom: "deconv_1/8d"
  top: "deconv_1/8d"
  name: "prelu_1/8d"
  type: "PReLU"
}
layer {
  name: "bn_1/8d"
  type: "BatchNorm"
  bottom: "deconv_1/8d"
  top: "deconv_1/8d"
  batch_norm_param{
    use_global_stats : true
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}



# Squeeze and Excitation for concat_3_12
layer {
  name: "pool_SE_1/8"
  type: "Pooling"
  bottom: "concat_3_12"
  top: "pool_SE_1/8"
  pooling_param {
    engine: CAFFE
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "conv_SE_1/8_1"
  type: "Convolution"
  bottom: "pool_SE_1/8"
  top: "conv_SE_1/8_1"
  param { lr_mult: 1 decay_mult: 1 }
  param { lr_mult: 2 decay_mult: 0 }
  convolution_param {
    num_output: 32
    kernel_size: 1
    stride: 1
    weight_filler { type: "gaussian" std: 0.01 } 
  }
}
layer {
  name: "relu_SE_1/8_1"
  type: "ReLU"
  bottom: "conv_SE_1/8_1"
  top: "conv_SE_1/8_1"
}
layer {
  name: "conv_SE_1/8_2"
  type: "Convolution"
  bottom: "conv_SE_1/8_1"
  top: "conv_SE_1/8_2"
  param { lr_mult: 1 decay_mult: 1 }
  param { lr_mult: 2 decay_mult: 0 }
  convolution_param {
    num_output: 512
    kernel_size: 1
    stride: 1
    weight_filler { type: "gaussian" std: 0.01 } 
  }
}
layer {
  name: "sigm_SE_1/8"
  type: "Sigmoid"
  bottom: "conv_SE_1/8_2"
  top: "sigm_SE_1/8"
}
layer {
  name: "reshape_SE_1/8"
  type: "Reshape"
  bottom: "sigm_SE_1/8"
  top: "reshape_SE_1/8"
  reshape_param {
    shape {
      dim: 0
      dim: 0
    }
  }
}
layer {
  name: "scale_SE_1/8"
  type: "Scale"
  bottom: "concat_3_12"
  bottom: "reshape_SE_1/8"
  top: "scale_SE_1/8"
  scale_param {
    axis: 0
    bias_term: false
  }
}
layer {
  name: "conv_SE_1/8"
  type: "Convolution"
  bottom: "scale_SE_1/8"
  top: "conv_SE_1/8"
  param { lr_mult: 1 decay_mult: 1 }
  param { lr_mult: 2 decay_mult: 0 }
  convolution_param {
    num_output: 128
    kernel_size: 1
    stride: 1
    weight_filler { type: "gaussian" std: 0.01 } 
  }
}
layer {
  bottom: "conv_SE_1/8"
  top: "conv_SE_1/8"
  name: "prelu_SE_1/8"
  type: "PReLU"
}
layer {
  name: "bn_SE_1/8"
  type: "BatchNorm"
  bottom: "conv_SE_1/8"
  top: "conv_SE_1/8"
  batch_norm_param{
    use_global_stats : true
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}


layer {
  name: "concat_1/8d"
  type: "Concat"
  bottom: "deconv_1/8d"
  bottom: "conv_SE_1/8"
  top: "concat_1/8d"
  concat_param {
    concat_dim: 1
  }
}
layer {
  bottom: "concat_1/8d"
  top: "conv_1/8_1d"
  name: "conv_1/8_1d"
  type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    bias_term: false
    weight_filler { type: "gaussian" std: 0.01 } 
  }
}
layer {
  bottom: "conv_1/8_1d"
  top: "conv_1/8_1d"
  name: "prelu_1/8_1d"
  type: "PReLU"
}
layer {
  name: "bn_1/8_1d"
  type: "BatchNorm"
  bottom: "conv_1/8_1d"
  top: "conv_1/8_1d"
  batch_norm_param{
    use_global_stats : true
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  bottom: "conv_1/8_1d"
  top: "conv_1/8_2d"
  name: "conv_1/8_2d"
  type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    bias_term: false
    weight_filler { type: "gaussian" std: 0.01 } 
  }
}
layer {
  bottom: "conv_1/8_2d"
  top: "conv_1/8_2d"
  name: "prelu_1/8_2d"
  type: "PReLU"
}
layer {
  name: "bn_1/8_2d"
  type: "BatchNorm"
  bottom: "conv_1/8_2d"
  top: "conv_1/8_2d"
  batch_norm_param{
    use_global_stats : true
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  bottom: "conv_1/8_2d"
  top: "conv_1/8_3d"
  name: "conv_1/8_3d"
  type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 64
    kernel_size: 1
    bias_term: false
    weight_filler { type: "gaussian" std: 0.01 } 
  }
}
layer {
  bottom: "conv_1/8_3d"
  top: "conv_1/8_3d"
  name: "prelu_1/8_3d"
  type: "PReLU"
}
layer {
  name: "bn_1/8_3d"
  type: "BatchNorm"
  bottom: "conv_1/8_3d"
  top: "conv_1/8_3d"
  batch_norm_param{
    use_global_stats : true
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
# 1/4
layer {
  bottom: "conv_1/8_3d"
  top: "deconv_1/4d"
  name: "deconv_1/4d"
  type: "Deconvolution"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 64
    kernel_size: 2
    stride: 2
    bias_term: false
    weight_filler { type: "gaussian" std: 0.01 } 
  }
}
layer {
  bottom: "deconv_1/4d"
  top: "deconv_1/4d"
  name: "prelu_1/4d"
  type: "PReLU"
}
layer {
  name: "bn_1/4d"
  type: "BatchNorm"
  bottom: "deconv_1/4d"
  top: "deconv_1/4d"
  batch_norm_param{
    use_global_stats : true
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}


# Squeeze and Excitation for concat_2_6
layer {
  name: "pool_SE_1/4"
  type: "Pooling"
  bottom: "concat_2_6"
  top: "pool_SE_1/4"
  pooling_param {
    engine: CAFFE
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "conv_SE_1/4_1"
  type: "Convolution"
  bottom: "pool_SE_1/4"
  top: "conv_SE_1/4_1"
  param { lr_mult: 1 decay_mult: 1 }
  param { lr_mult: 2 decay_mult: 0 }
  convolution_param {
    num_output: 16
    kernel_size: 1
    stride: 1
    weight_filler { type: "gaussian" std: 0.01 } 
  }
}
layer {
  name: "relu_SE_1/4_1"
  type: "ReLU"
  bottom: "conv_SE_1/4_1"
  top: "conv_SE_1/4_1"
}
layer {
  name: "conv_SE_1/4_2"
  type: "Convolution"
  bottom: "conv_SE_1/4_1"
  top: "conv_SE_1/4_2"
  param { lr_mult: 1 decay_mult: 1 }
  param { lr_mult: 2 decay_mult: 0 }
  convolution_param {
    num_output: 256
    kernel_size: 1
    stride: 1
    weight_filler { type: "gaussian" std: 0.01 } 
  }
}
layer {
  name: "sigm_SE_1/4"
  type: "Sigmoid"
  bottom: "conv_SE_1/4_2"
  top: "sigm_SE_1/4"
}
layer {
  name: "reshape_SE_1/4"
  type: "Reshape"
  bottom: "sigm_SE_1/4"
  top: "reshape_SE_1/4"
  reshape_param {
    shape {
      dim: 0
      dim: 0
    }
  }
}
layer {
  name: "scale_SE_1/4"
  type: "Scale"
  bottom: "concat_2_6"
  bottom: "reshape_SE_1/4"
  top: "scale_SE_1/4"
  scale_param {
    axis: 0
    bias_term: false
  }
}
layer {
  name: "conv_SE_1/4"
  type: "Convolution"
  bottom: "scale_SE_1/4"
  top: "conv_SE_1/4"
  param { lr_mult: 1 decay_mult: 1 }
  param { lr_mult: 2 decay_mult: 0 }
  convolution_param {
    num_output: 64
    kernel_size: 1
    stride: 1
    weight_filler { type: "gaussian" std: 0.01 } 
  }
}
layer {
  bottom: "conv_SE_1/4"
  top: "conv_SE_1/4"
  name: "prelu_SE_1/4"
  type: "PReLU"
}
layer {
  name: "bn_SE_1/4"
  type: "BatchNorm"
  bottom: "conv_SE_1/4"
  top: "conv_SE_1/4"
  batch_norm_param{
    use_global_stats : true
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}



layer {
  name: "concat_1/4d"
  type: "Concat"
  bottom: "deconv_1/4d"
  bottom: "conv_SE_1/4"
  top: "concat_1/4d"
  concat_param {
    concat_dim: 1
  }
}
layer {
  bottom: "concat_1/4d"
  top: "conv_1/4_1d"
  name: "conv_1/4_1d"
  type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    bias_term: false
    weight_filler { type: "gaussian" std: 0.01 } 
  }
}
layer {
  bottom: "conv_1/4_1d"
  top: "conv_1/4_1d"
  name: "prelu_1/4_1d"
  type: "PReLU"
}
layer {
  name: "bn_1/4_1d"
  type: "BatchNorm"
  bottom: "conv_1/4_1d"
  top: "conv_1/4_1d"
  batch_norm_param{
    use_global_stats : true
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  bottom: "conv_1/4_1d"
  top: "conv_1/4_2d"
  name: "conv_1/4_2d"
  type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    bias_term: false
    weight_filler { type: "gaussian" std: 0.01 } 
  }
}
layer {
  bottom: "conv_1/4_2d"
  top: "conv_1/4_2d"
  name: "prelu_1/4_2d"
  type: "PReLU"
}
layer {
  name: "bn_1/4_2d"
  type: "BatchNorm"
  bottom: "conv_1/4_2d"
  top: "conv_1/4_2d"
  batch_norm_param{
    use_global_stats : true
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  bottom: "conv_1/4_2d"
  top: "conv_1/4_3d"
  name: "conv_1/4_3d"
  type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 32
    kernel_size: 1
    bias_term: false
    weight_filler { type: "gaussian" std: 0.01 } 
  }
}
layer {
  bottom: "conv_1/4_3d"
  top: "conv_1/4_3d"
  name: "prelu_1/4_3d"
  type: "PReLU"
}
layer {
  name: "bn_1/4_3d"
  type: "BatchNorm"
  bottom: "conv_1/4_3d"
  top: "conv_1/4_3d"
  batch_norm_param{
    use_global_stats : true
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}

########################################################################
# Prediction at 1/4
########################################################################
layer {
  bottom: "conv_1/4_3d"
  top: "pred_1/4"
  name: "pred_1/4"
  type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 }
  param { lr_mult: 2 decay_mult: 0 }
  convolution_param {
    num_output: 1
    kernel_size: 1
    weight_filler { type: "gaussian" std: 0.01 } 
  }
}

########################################################################
# Upsample the prediction from 1/4 to 1/1
########################################################################
layer {
  bottom: "pred_1/4"
  top: "pred_step_1"
  name: "pred_step_1"
  type: "Deconvolution"
  param { lr_mult: 1 decay_mult: 1 }
  convolution_param {
    num_output: 1
    kernel_size: 4
    stride: 4
    bias_term: false
    weight_filler { type: "gaussian" std: 0.01 } 
  }
}




layer {
  name: "sigp_step_1"
  type: "Sigmoid"
  bottom: "pred_step_1"
  top: "sigp_step_1"
}






















############################################################################################
# Secondary network
############################################################################################
layer {
  name: "concat_step_1"
  bottom: "concat_input"
  bottom: "sigp_step_1"
  top: "concat_step_1"
  type: "Concat"
  concat_param {
    concat_dim: 1
  }
}

############################################################################################

layer {
  bottom: "concat_step_1"
  top: "conv_atrous1_1"
  name: "conv_atrous1_1"
  type: "Convolution"
  param { lr_mult: 100 decay_mult: 1 }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    bias_term: false
    weight_filler { type: "gaussian" std: 0.01 } 
  }
}
layer {
  bottom: "conv_atrous1_1"
  top: "conv_atrous1_1"
  name: "prelu_atrous1_1"
  type: "PReLU"
}
layer {
  name: "bn_atrous1_1"
  type: "BatchNorm"
  bottom: "conv_atrous1_1"
  top: "conv_atrous1_1"
  batch_norm_param{
    use_global_stats : true
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  bottom: "conv_atrous1_1"
  top: "conv_atrous1_2"
  name: "conv_atrous1_2"
  type: "Convolution"
  param { lr_mult: 100 decay_mult: 1 }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    bias_term: false
    weight_filler { type: "gaussian" std: 0.01 } 
  }
}
layer {
  bottom: "conv_atrous1_2"
  top: "conv_atrous1_2"
  name: "prelu_atrous1_2"
  type: "PReLU"
}
layer {
  name: "bn_atrous1_2"
  type: "BatchNorm"
  bottom: "conv_atrous1_2"
  top: "conv_atrous1_2"
  batch_norm_param{
    use_global_stats : true
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  bottom: "conv_atrous1_2"
  top: "conv_atrous1_3"
  name: "conv_atrous1_3"
  type: "Convolution"
  param { lr_mult: 100 decay_mult: 1 }
  convolution_param {
    num_output: 16
    kernel_size: 1
    bias_term: false
    weight_filler { type: "gaussian" std: 0.01 } 
  }
}
layer {
  bottom: "conv_atrous1_3"
  top: "conv_atrous1_3"
  name: "prelu_atrous1_3"
  type: "PReLU"
}
layer {
  name: "bn_atrous1_3"
  type: "BatchNorm"
  bottom: "conv_atrous1_3"
  top: "conv_atrous1_3"
  batch_norm_param{
    use_global_stats : true
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}

############################################################################################

layer {
  bottom: "concat_step_1"
  top: "conv_atrous2_1"
  name: "conv_atrous2_1"
  type: "Convolution"
  param { lr_mult: 100 decay_mult: 1 }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 3
    dilation: 2
    bias_term: false
    weight_filler { type: "gaussian" std: 0.01 } 
  }
}
layer {
  bottom: "conv_atrous2_1"
  top: "conv_atrous2_1"
  name: "prelu_atrous2_1"
  type: "PReLU"
}
layer {
  name: "bn_atrous2_1"
  type: "BatchNorm"
  bottom: "conv_atrous2_1"
  top: "conv_atrous2_1"
  batch_norm_param{
    use_global_stats : true
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  bottom: "conv_atrous2_1"
  top: "conv_atrous2_2"
  name: "conv_atrous2_2"
  type: "Convolution"
  param { lr_mult: 100 decay_mult: 1 }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    bias_term: false
    weight_filler { type: "gaussian" std: 0.01 } 
  }
}
layer {
  bottom: "conv_atrous2_2"
  top: "conv_atrous2_2"
  name: "prelu_atrous2_2"
  type: "PReLU"
}
layer {
  name: "bn_atrous2_2"
  type: "BatchNorm"
  bottom: "conv_atrous2_2"
  top: "conv_atrous2_2"
  batch_norm_param{
    use_global_stats : true
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  bottom: "conv_atrous2_2"
  top: "conv_atrous2_3"
  name: "conv_atrous2_3"
  type: "Convolution"
  param { lr_mult: 100 decay_mult: 1 }
  convolution_param {
    num_output: 16
    kernel_size: 1
    bias_term: false
    weight_filler { type: "gaussian" std: 0.01 } 
  }
}
layer {
  bottom: "conv_atrous2_3"
  top: "conv_atrous2_3"
  name: "prelu_atrous2_3"
  type: "PReLU"
}
layer {
  name: "bn_atrous2_3"
  type: "BatchNorm"
  bottom: "conv_atrous2_3"
  top: "conv_atrous2_3"
  batch_norm_param{
    use_global_stats : true
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}

############################################################################################

layer {
  bottom: "concat_step_1"
  top: "conv_atrous3_1"
  name: "conv_atrous3_1"
  type: "Convolution"
  param { lr_mult: 100 decay_mult: 1 }
  convolution_param {
    num_output: 32
    pad: 3
    kernel_size: 3
    dilation: 3
    bias_term: false
    weight_filler { type: "gaussian" std: 0.01 } 
  }
}
layer {
  bottom: "conv_atrous3_1"
  top: "conv_atrous3_1"
  name: "prelu_atrous3_1"
  type: "PReLU"
}
layer {
  name: "bn_atrous3_1"
  type: "BatchNorm"
  bottom: "conv_atrous3_1"
  top: "conv_atrous3_1"
  batch_norm_param{
    use_global_stats : true
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  bottom: "conv_atrous3_1"
  top: "conv_atrous3_2"
  name: "conv_atrous3_2"
  type: "Convolution"
  param { lr_mult: 100 decay_mult: 1 }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    bias_term: false
    weight_filler { type: "gaussian" std: 0.01 } 
  }
}
layer {
  bottom: "conv_atrous3_2"
  top: "conv_atrous3_2"
  name: "prelu_atrous3_2"
  type: "PReLU"
}
layer {
  name: "bn_atrous3_2"
  type: "BatchNorm"
  bottom: "conv_atrous3_2"
  top: "conv_atrous3_2"
  batch_norm_param{
    use_global_stats : true
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  bottom: "conv_atrous3_2"
  top: "conv_atrous3_3"
  name: "conv_atrous3_3"
  type: "Convolution"
  param { lr_mult: 100 decay_mult: 1 }
  convolution_param {
    num_output: 16
    kernel_size: 1
    bias_term: false
    weight_filler { type: "gaussian" std: 0.01 } 
  }
}
layer {
  bottom: "conv_atrous3_3"
  top: "conv_atrous3_3"
  name: "prelu_atrous3_3"
  type: "PReLU"
}
layer {
  name: "bn_atrous3_3"
  type: "BatchNorm"
  bottom: "conv_atrous3_3"
  top: "conv_atrous3_3"
  batch_norm_param{
    use_global_stats : true
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}

############################################################################################
# Concatenation + Squeeze & Excitation
############################################################################################

layer {
  name: "concat_step_2"
  bottom: "conv_atrous1_3"
  bottom: "conv_atrous2_3"
  bottom: "conv_atrous3_3"
  top: "concat_step_2"
  type: "Concat"
  concat_param {
    concat_dim: 1
  }
}

layer {
  name: "gpool_s2"
  type: "Pooling"
  bottom: "concat_step_2"
  top: "gpool_s2"
  pooling_param {
    engine: CAFFE
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "conv_s2_down"
  type: "Convolution"
  bottom: "gpool_s2"
  top: "conv_s2_down"
  param { lr_mult: 100 decay_mult: 1 }
  param { lr_mult: 200 decay_mult: 0 }
  convolution_param {
    num_output: 3
    kernel_size: 1
    stride: 1
    weight_filler { type: "gaussian" std: 0.01 } 
  }
}
layer {
  name: "relu_s2_down"
  type: "ReLU"
  bottom: "conv_s2_down"
  top: "conv_s2_down"
}
layer {
  name: "conv_s2_up"
  type: "Convolution"
  bottom: "conv_s2_down"
  top: "conv_s2_up"
  param { lr_mult: 100 decay_mult: 1 }
  param { lr_mult: 200 decay_mult: 0 }
  convolution_param {
    num_output: 48
    kernel_size: 1
    stride: 1
    weight_filler { type: "gaussian" std: 0.01 } 
  }
}
layer {
  name: "sig_s2_up"
  type: "Sigmoid"
  bottom: "conv_s2_up"
  top: "conv_s2_up"
}
layer {
  name: "resh_s2"
  type: "Reshape"
  bottom: "conv_s2_up"
  top: "resh_s2"
  reshape_param {
    shape {
      dim: 0
      dim: 0
    }
  }
}
layer {
  name: "scale_s2"
  type: "Scale"
  bottom: "concat_step_2"
  bottom: "resh_s2"
  top: "scale_s2"
  scale_param {
    axis: 0
    bias_term: false
  }
}




############################################################################################
# Prediction
############################################################################################

layer {
  bottom: "scale_s2"
  top: "conv_p1_1"
  name: "conv_p1_1"
  type: "Convolution"
  param { lr_mult: 100 decay_mult: 1 }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    bias_term: false
    weight_filler { type: "gaussian" std: 0.01 } 
  }
}
layer {
  bottom: "conv_p1_1"
  top: "conv_p1_1"
  name: "prelu_p1_1"
  type: "PReLU"
}
layer {
  name: "bn_p1_1"
  type: "BatchNorm"
  bottom: "conv_p1_1"
  top: "conv_p1_1"
  batch_norm_param{
    use_global_stats : true
  }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  bottom: "conv_p1_1"
  top: "pred_step_2"
  name: "pred_step_2"
  type: "Convolution"
  param { lr_mult: 100 decay_mult: 1 }
  param { lr_mult: 200 decay_mult: 0 }
  convolution_param {
    num_output: 1
    kernel_size: 1
    weight_filler { type: "gaussian" std: 0.01 } 
  }
}


###################################################
# Prediction
###################################################
layer {
  name: "sig_pred"
  type: "Sigmoid"
  bottom: "pred_step_2"
  top: "sig_pred"
}
